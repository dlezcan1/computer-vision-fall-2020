{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": " GAN_lab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF2A682ZP3qN"
      },
      "source": [
        "# Generative Adversarial Networks (GANs)\n",
        "\n",
        "In this lab, we will be working with GANs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wM3JjZaP3qO"
      },
      "source": [
        "!pip3 install torch torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpxlY9wGP3qO"
      },
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision.models.inception import inception_v3\n",
        "from torchvision import datasets, transforms\n",
        "from scipy.stats import entropy\n",
        "import copy\n",
        "\n",
        "import itertools\n",
        "\n",
        "torch.manual_seed(0); #you may want to make use of this in various cells for reproducability\n",
        "gpu_boole = torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-xgpveAP3qO"
      },
      "source": [
        "## 1. Introduction and Motivation\n",
        "\n",
        "As a recap from lecture:\n",
        "\n",
        "GANs are a popular architecture class for\n",
        "generating data first introduced in (Goodfellow et al 2014). \n",
        "\n",
        "The two major components of a GAN framework are a Discriminator network $D$ and\n",
        "a Generator network $G$.\n",
        "\n",
        "$D$ takes a data sample as input and decides whether it is real (coming from the actual \n",
        "training set) or fake (generated artificially). $G$ takes some random noise vector\n",
        "$z$ as input and outputs a generated sample. \n",
        "\n",
        "In practice, these networks \"battle\": $G$ continually attmepts to generate\n",
        "more realistic samples and $D$ continually tries to get better at distinguishing\n",
        "real samples from fake smaples. This can be formulated as a min-max zero sum game:\n",
        "\n",
        "$$\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)} [\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)} [\\log 1 - D(G(z))]$$\n",
        "where $p_z(z)$ denotes a defined prior over the noise vector. Local and global minima for this problem exist at local and global Nash Equilibria.\n",
        "\n",
        "The hope is that, over training, $D$ becomes a very good discriminator and $G$ becomes\n",
        "a very good generator. At the end of training, we can then make use of $G$\n",
        "to generate nice looking samples that seem like they were drawn from the original\n",
        "distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddJ2O425P3qO"
      },
      "source": [
        "## 2. GAN for Gaussian Distribution\n",
        "\n",
        "As an introduction, we will define and train a GAN that can replicate a simple\n",
        "Gaussian distribution.\n",
        "\n",
        "Let's start by generating random samples from a Gaussian distribution and plotting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSzHB9GiP3qO"
      },
      "source": [
        "N = 1500\n",
        "x = torch.randn(N)\n",
        "x = x.view(-1,1)\n",
        "\n",
        "#Plotting:\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(x.cpu().data.numpy(),'o',markersize=0.5)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(x.cpu().data.numpy());\n",
        "print(x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb6MofEmP3qO"
      },
      "source": [
        "Now, let's define D and G networks as described in the introduction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEHLS7C3P3qO"
      },
      "source": [
        "First, let's define the generator network $G$. \n",
        "We want a network which maps $k$-dimensional noise vectors to samples\n",
        "from the Gaussian distribution. In other words, we want a mapping\n",
        "$\\mathbb{R}^k \\rightarrow \\mathbb{R}$. Define a simple, small MLP which performs this mapping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUaunxlMP3qO"
      },
      "source": [
        "class generator_mlp(nn.Module):\n",
        "    # initializers\n",
        "    def __init__(self, k):\n",
        "        super(generator_mlp, self).__init__()\n",
        "        #TO-DO\n",
        "        \n",
        "    # forward method\n",
        "    def forward(self, input):\n",
        "        #TO-DO\n",
        "\n",
        "        \n",
        "k=10 #feel free to change k\n",
        "G = generator_mlp(k)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD4DikDOP3qO"
      },
      "source": [
        "Next, let's define the discriminator network $D$. \n",
        "We want a network which maps a candidate sample, possibly from the Gaussian \n",
        "distribution to the probability of that candidate sample belonging to the \n",
        "true Gaussian distribution. In other words, we want a mapping\n",
        "$\\mathbb{R} \\rightarrow [0,1]$. Define a simple, small MLP which performs this mapping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb6XMuDgP3qO"
      },
      "source": [
        "class discriminator_mlp(nn.Module):\n",
        "    # initializers\n",
        "    def __init__(self):\n",
        "        super(discriminator_mlp, self).__init__()\n",
        "        #TO-DO\n",
        "\n",
        "    # forward method\n",
        "    def forward(self, input):\n",
        "        #TO-DO\n",
        "    \n",
        "D = discriminator_mlp()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw2-5XGGP3qO"
      },
      "source": [
        "### 2.1. Training the GAN\n",
        "\n",
        "Now, we will set up the $D, G$ training regime. We iterate over our training\n",
        "set (the Gaussian distribution samples). For each batch:\n",
        "\n",
        "1. For the discriminator, push $D$ towards identifying each sample in the batch as probability close to 1 (i.e. belonging to the true distribution). We then generate an equal number of artificial samples with $G$ and push $D$ towards identifying each sample in the batch as probability close to 0 (i.e. that it is a fake sample). Both of these are done through minimizing BCE loss.\n",
        "2. For $G$, we will take another random batch of artificial samples and update $G$ in such a way that we push $D$ towards identifying the artificial samples as real. This is done through maximizing BCE loss.\n",
        "\n",
        "In our case, we train over 50 epochs with a batch size of 32.\n",
        "\n",
        "#### Deliverables:\n",
        "\n",
        "While keeping epochs and batch size fixed, modify the optimizer and your\n",
        "$D$ and $G$ architectures such that the samples from $G$ approximate the curve\n",
        "\"well.\" **After training, the mean and variance of your G samples should be within 0.15 of the mean and variance of th true distribution.**\n",
        "\n",
        "For this seciton, $D$ and $G$ **architectures are restricted to MLPs only**\n",
        "(No restrictions on regularization methods though.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "c1ILKUvjP3qO"
      },
      "source": [
        "epochs = 50 #keep fixed\n",
        "batch_size = 32 #keep fixed\n",
        "lr = 0.2\n",
        "\n",
        "G = generator_mlp(k)\n",
        "D = discriminator_mlp()\n",
        "\n",
        "#data loader:\n",
        "train = torch.utils.data.TensorDataset(x)\n",
        "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "#D,G optimizers:\n",
        "G_optimizer = optim.SGD(G.parameters(), lr=lr)\n",
        "D_optimizer = optim.SGD(D.parameters(), lr=lr)\n",
        "\n",
        "#loss definition(s):\n",
        "BCE_loss = nn.BCELoss()\n",
        "\n",
        "#training loop:\n",
        "D_losses = []\n",
        "G_losses = []\n",
        "for epoch in range(epochs):\n",
        "    for x_ in train_loader:\n",
        "\n",
        "        x_ = x_[0]\n",
        "        \n",
        "        # train discriminator D\n",
        "        D.zero_grad()\n",
        "\n",
        "        mini_batch = x_.size()[0]\n",
        "\n",
        "        y_real_ = torch.ones(mini_batch)\n",
        "        y_fake_ = torch.zeros(mini_batch)\n",
        "\n",
        "        x_, y_real_, y_fake_ = Variable(x_), Variable(y_real_), Variable(y_fake_)\n",
        "        D_result = D(x_)\n",
        "        D_real_loss = BCE_loss(D_result, y_real_)\n",
        "\n",
        "        z_ = torch.randn((mini_batch, k))\n",
        "        \n",
        "        z_ = Variable(z_)\n",
        "        G_result = G(z_)\n",
        "\n",
        "        D_result = D(G_result)\n",
        "        D_fake_loss = BCE_loss(D_result, y_fake_)\n",
        "        D_fake_score = D_result.data.mean()\n",
        "\n",
        "        D_train_loss = D_real_loss + D_fake_loss\n",
        "\n",
        "        D_train_loss.backward()\n",
        "        D_optimizer.step()\n",
        "        \n",
        "        D_losses.append(D_train_loss.data.item())\n",
        "        \n",
        "        # train generator G\n",
        "        G.zero_grad()\n",
        "\n",
        "        z_ = torch.randn((mini_batch, k))\n",
        "        z_ = Variable(z_)\n",
        "\n",
        "        G_result = G(z_)\n",
        "        D_result = D(G_result).squeeze()\n",
        "        G_train_loss = BCE_loss(D_result, y_real_)\n",
        "\n",
        "        G_train_loss.backward()\n",
        "        G_optimizer.step()\n",
        "\n",
        "        G_losses.append(G_train_loss.data.item())\n",
        "    \n",
        "    if ((epoch+1)%10==0):\n",
        "        print('[%d/%d] - loss_d: %.3f, loss_g: %.3f' % ((epoch + 1), epochs, torch.mean(torch.FloatTensor(D_losses)),\n",
        "                                                                  torch.mean(torch.FloatTensor(G_losses))))\n",
        "\n",
        "#Plotting:\n",
        "\n",
        "#Losses:\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(D_losses)\n",
        "plt.title(\"D Loss\")\n",
        "plt.xlabel(\"Batch number\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(G_losses)\n",
        "plt.title(\"G Loss\")\n",
        "plt.xlabel(\"Batch number\")\n",
        "plt.ylabel(\"Loss\");\n",
        "\n",
        "#Samples from G:\n",
        "z_ = torch.randn((1500, k))\n",
        "z_ = Variable(z_)\n",
        "\n",
        "G_result = G(z_)\n",
        "\n",
        "print()\n",
        "print('Generator statistics:')\n",
        "print('Mean:',G_result.cpu().data.numpy().mean(),'Var:',G_result.cpu().data.numpy().std())\n",
        "print('True distribution statistics:')\n",
        "print('Mean:',x.cpu().data.numpy().mean(),'Var:',x.cpu().data.numpy().std())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug3GJNRXP3qO"
      },
      "source": [
        "For these loss plots, you'd like to see some periodic oscillation for both nets, followed by convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-WukOxxP3qO"
      },
      "source": [
        "##### Visualizing the samples:\n",
        "\n",
        "These cells visualize the $G$ samples and the original dataset. For visualization purposes, you'd like to see the spread and histograms to match somewhat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hadz_nkpP3qO"
      },
      "source": [
        "#Samples from G:\n",
        "z_ = torch.randn((1500, k))\n",
        "z_ = Variable(z_)\n",
        "\n",
        "G_result = G(z_)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(G_result.cpu().data.numpy(),'o',markersize=0.5)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(G_result.cpu().data.numpy());"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyJ6kQ6qP3qO"
      },
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(x.cpu().data.numpy(),'o',markersize=0.5)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(x.cpu().data.numpy());\n",
        "print(x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5EpzTB6P3qO"
      },
      "source": [
        "## 3. Experiments on MNIST\n",
        "\n",
        "Now we move on to MNIST experiments.\n",
        "\n",
        "### 3.1. Inception Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4Gb-AhYP3qO"
      },
      "source": [
        "Rather than just eye-balling whether GAN samples look good or not,\n",
        "researchers have come up with objective metrics for determining the quality\n",
        "and breadth of GAN outputs. One such metric is called the inception score (IS),\n",
        "which we will be using, although it is not without its drawbacks. It can be implemented in multiple ways. The higher the inception score, the better our GAN model is.\n",
        "\n",
        "A detailed and thorough explanation of IS can be found from multiple sources. Roughly stated, for a dataset with a given number of classes, high inception scores correspond to GAN samples that have low intra-class entropy and high inter-class entropy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U51Pv7gnP3qO"
      },
      "source": [
        "def MNIST_IS(imgs, model_path='MNIST.ckpt', batch_size=32, splits=10):\n",
        "    \n",
        "    \"\"\"Computes the inception score of the generated images imgs\n",
        "    imgs -- numpy array of dimension (# of datapoints, 1, 28, 28)\n",
        "    batch_size -- batch size for feeding into the pretrained MNIST model\n",
        "    splits -- number of splits\n",
        "    \"\"\"\n",
        "    N = len(imgs)\n",
        "    \n",
        "    #check imgs is numpy array & of desired dimension\n",
        "    assert type(imgs) is np.ndarray\n",
        "    assert imgs.shape[1] == 1\n",
        "    assert imgs.shape[2] == 28\n",
        "    assert imgs.shape[3] == 28\n",
        "    assert batch_size > 0\n",
        "    assert N > batch_size\n",
        "    \n",
        "    imgs = copy.copy(imgs)\n",
        "    \n",
        "    # Set up dtype\n",
        "    if torch.cuda.is_available():\n",
        "        dtype = torch.cuda.FloatTensor\n",
        "    else:\n",
        "        dtype = torch.FloatTensor\n",
        "    \n",
        "    MEAN = 0.1307\n",
        "    STD = 0.3081\n",
        "    imgs -= MEAN\n",
        "    imgs /= STD\n",
        "    \n",
        "    # Set up dataloader\n",
        "    dataloader = torch.utils.data.DataLoader(imgs, batch_size=batch_size)\n",
        "\n",
        "    # Load inception model\n",
        "    MNIST_model = ConvNet()\n",
        "    MNIST_model.load_state_dict(torch.load(model_path))\n",
        "    if torch.cuda.is_available():\n",
        "        MNIST_model = MNIST_model.cuda()\n",
        "    MNIST_model.eval()\n",
        "\n",
        "    def get_pred(x):\n",
        "        if torch.cuda.is_available():\n",
        "            x = x.cuda()\n",
        "        x = MNIST_model(x)\n",
        "        return F.softmax(x, dim=0).data.cpu().numpy()\n",
        "\n",
        "    # Get predictions\n",
        "    preds = np.zeros((N, 10))\n",
        "\n",
        "    for i, batch in enumerate(dataloader, 0):\n",
        "        batch = batch.type(dtype)\n",
        "        batchv = Variable(batch)\n",
        "        batch_size_i = batch.size()[0]\n",
        "\n",
        "        preds[i*batch_size:i*batch_size + batch_size_i] = get_pred(batchv)\n",
        "\n",
        "    # Now compute the mean kl-div\n",
        "    split_scores = []\n",
        "\n",
        "    for k in range(splits):\n",
        "        part = preds[k * (N // splits): (k+1) * (N // splits), :]\n",
        "        py = np.mean(part, axis=0)\n",
        "        scores = []\n",
        "        for i in range(part.shape[0]):\n",
        "            pyx = part[i, :]\n",
        "            scores.append(entropy(pyx, py))\n",
        "        split_scores.append(np.exp(np.mean(scores)))\n",
        "    \n",
        "    return np.mean(split_scores), np.std(split_scores)\n",
        "\n",
        "#Model structure of the pretrained MNIST data\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.drop_out = nn.Dropout()\n",
        "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.drop_out(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsyZJXAOP3qO"
      },
      "source": [
        "Now, let's see the inception score for the actual MNIST dataset.\n",
        "\n",
        "You will need to make sure the provided `MNIST.ckpt` file is in your working directory or specify its path via the `model_path` argument of `MNIST_IS`. If using Google Colab, click `View` --> `Table of Contents` --> `Files` and then upload the `MNIST.ckpt` file.\n",
        "\n",
        "The cell below gives an IS estimate for some MNIST samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlXCEGNBP3qO"
      },
      "source": [
        "transform = transforms.ToTensor()\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('./data', train=True, download=True, transform=transform),\n",
        "    batch_size=500, shuffle=True)\n",
        "\n",
        "for x, y in train_loader:\n",
        "    x = x\n",
        "    break\n",
        "x = x.cpu().data.numpy()\n",
        "x = x.reshape([-1,1,28,28])\n",
        "print('Shape of data:',x.shape)\n",
        "mis = MNIST_IS(x)\n",
        "print('Inception Score:','mean:',mis[0],'std:',mis[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPpHfMZ_P3qO"
      },
      "source": [
        "The score for MNIST should be somewhere around 8 to 12. As we train our GAN(s), we will be looking to\n",
        "get as high an inception score as possible on our generated images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjoMze3uP3qO"
      },
      "source": [
        "### 3.2. Generating MNIST images\n",
        "\n",
        "As you did with the Gaussian distribution, see if you can train D and G\n",
        "such that G can generate nice images for MNIST. **You are free to use any architecture class, but keep it fairly computationally inexpensive, if possible.** Simple CNNs are good guidelines for computational tractability.\n",
        "\n",
        "#### Deliverables\n",
        "\n",
        "Given the limited computational resources, you will want to achieve an inception score of 1.5 or greater for full credit. An IS of 1.5 won't yield great images. For nice looking images, you'll need an inception score of around 6.0, but it is not needed for full credit.\n",
        "\n",
        "Scores higher than 1.5 will receive bonus points.\n",
        "\n",
        "#### Instructions for long training times\n",
        "\n",
        "For more complicated architectures, if your model takes a long time to train, you will need to save the model and write a code snippet that loads it such that the code runs with no errors and we can grade it expediently. In this case, set epochs = 0 and include the saved model in your submission on blackboard (or a Google drive link / something similar if it is too large for BB).\n",
        "\n",
        "#### Some Optional Tips\n",
        "\n",
        "It will be easier to get better results with a convolutional GAN as MLPs are typically not used in practical settings. Deconvolutional layers (implemented via `nn.ConvTranspose2d` in pytorch) are typically used. I suggest paying attention to the shapes at each line of the forward pass to avoid errors.\n",
        "\n",
        "There are additional GAN architectures online you may want to reference for inspiration. (But do not plagiarize -- please write your own custom network.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls6YqVmyP3qO"
      },
      "source": [
        "#Defining your networks:\n",
        "\n",
        "class generator(nn.Module):\n",
        "    # initializers\n",
        "    def __init__(self,k, d=2):\n",
        "        super(generator, self).__init__()\n",
        "        #TO-DO\n",
        "        \n",
        "    # weight_init\n",
        "    def weight_init(self, mean, std):\n",
        "        for m in self._modules:\n",
        "            normal_init(self._modules[m], mean, std)\n",
        "\n",
        "    # forward method\n",
        "    def forward(self, input):\n",
        "        #TO-DO\n",
        "        \n",
        "        \n",
        "k=10 #feel free to change k\n",
        "\n",
        "class discriminator(nn.Module):\n",
        "    # initializers\n",
        "    def __init__(self, d=2):\n",
        "        super(discriminator, self).__init__()\n",
        "        #TO-DO\n",
        "\n",
        "    # weight_init\n",
        "    def weight_init(self, mean, std):\n",
        "        for m in self._modules:\n",
        "            normal_init(self._modules[m], mean, std)\n",
        "\n",
        "    # forward method\n",
        "    def forward(self, input):\n",
        "        #TO-DO\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjcpIibwP3qO"
      },
      "source": [
        "The training code is below. Feel free to modify  it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "kHV6sMbtP3qP"
      },
      "source": [
        "#Training code:\n",
        "\n",
        "gpu_boole = torch.cuda.is_available()\n",
        "cnn_boole = False #set True for CNN reshaping. False otherwise.\n",
        "epochs = 1\n",
        "batch_size = 32 \n",
        "lr = 0.2\n",
        "\n",
        "# choose your own d\n",
        "G = generator(k, d=4)\n",
        "D = discriminator(d=4)\n",
        "\n",
        "if gpu_boole:\n",
        "    G = G.cuda()\n",
        "    D = D.cuda()\n",
        "    \n",
        "#data loader:\n",
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=0.5, std=0.5)\n",
        "])\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('./data', train=True, download=True, transform=transform),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "\n",
        "#D,G optimizers:\n",
        "G_optimizer = optim.SGD(G.parameters(), lr=lr)\n",
        "D_optimizer = optim.SGD(D.parameters(), lr=lr)\n",
        "\n",
        "#loss definition(s):\n",
        "BCE_loss = nn.BCELoss()\n",
        "\n",
        "#training loop:\n",
        "D_losses = []\n",
        "G_losses = []\n",
        "print(\"Training start!\")\n",
        "for epoch in range(epochs):\n",
        "    for x_, _ in train_loader:\n",
        "        \n",
        "        #reshaping depending on your architecture class:\n",
        "        if not cnn_boole:\n",
        "            x_ = x_.view(batch_size,-1) #this reshape is needed for MLP class\n",
        "        if gpu_boole:\n",
        "            x_ = x_.cuda()\n",
        "        \n",
        "        # train discriminator D\n",
        "        D.zero_grad()\n",
        "\n",
        "        mini_batch = x_.size()[0]\n",
        "\n",
        "        y_real_ = torch.ones(mini_batch)\n",
        "        y_fake_ = torch.zeros(mini_batch)\n",
        "        if gpu_boole:\n",
        "            y_real_ = y_real_.cuda()\n",
        "            y_fake_ = y_fake_.cuda()\n",
        "\n",
        "        x_, y_real_, y_fake_ = Variable(x_), Variable(y_real_), Variable(y_fake_)\n",
        "        D_result = D(x_)\n",
        "        D_real_loss = BCE_loss(D_result, y_real_)\n",
        "\n",
        "        z_ = torch.randn((mini_batch, k))\n",
        "        if cnn_boole:\n",
        "            z_ = z_.view(-1, k, 1, 1) #needed for CNN        \n",
        "        z_ = Variable(z_)\n",
        "        if gpu_boole:\n",
        "            z_ = z_.cuda()\n",
        "\n",
        "        G_result = G(z_)\n",
        "        D_result = D(G_result)\n",
        "        D_fake_loss = BCE_loss(D_result, y_fake_)\n",
        "        D_fake_score = D_result.data.mean()\n",
        "\n",
        "        D_train_loss = D_real_loss + D_fake_loss\n",
        "\n",
        "        D_train_loss.backward()\n",
        "        D_optimizer.step()\n",
        "        \n",
        "        D_losses.append(D_train_loss.data.item())\n",
        "        \n",
        "        # train generator G\n",
        "        G.zero_grad()\n",
        "\n",
        "        z_ = torch.randn((mini_batch, k))\n",
        "        if cnn_boole:\n",
        "            z_ = z_.view(-1, k, 1, 1) #needed for CNN\n",
        "        z_ = Variable(z_)\n",
        "        if gpu_boole:\n",
        "            z_ = z_.cuda()\n",
        "\n",
        "        G_result = G(z_)\n",
        "        D_result = D(G_result).squeeze()\n",
        "        G_train_loss = BCE_loss(D_result, y_real_)\n",
        "\n",
        "        G_train_loss.backward()\n",
        "        G_optimizer.step()\n",
        "\n",
        "        G_losses.append(G_train_loss.data.item())\n",
        "    \n",
        "    print('[%d/%d] - loss_d: %.3f, loss_g: %.3f' % ((epoch + 1), epochs, torch.mean(torch.FloatTensor(D_losses)),\n",
        "                                                                  torch.mean(torch.FloatTensor(G_losses))))\n",
        "\n",
        "#Plotting:\n",
        "\n",
        "#Losses:\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(D_losses)\n",
        "plt.title(\"D Loss\")\n",
        "plt.xlabel(\"Batch number\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(G_losses)\n",
        "plt.title(\"G Loss\")\n",
        "plt.xlabel(\"Batch number\")\n",
        "plt.ylabel(\"Loss\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-htpu1ZP3qP"
      },
      "source": [
        "##### Computing inception score:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "693S8soFP3qP"
      },
      "source": [
        "# Samples from G:\n",
        "z_ = torch.randn((1500, k))\n",
        "z_ = Variable(z_)\n",
        "if gpu_boole:\n",
        "    z_ = z_.cuda()\n",
        "if cnn_boole:\n",
        "    z_ = z_.view(-1, k, 1, 1) #needed for CNN        \n",
        "    \n",
        "G_result = G(z_)\n",
        "G_result = G_result.cpu().data.numpy()\n",
        "G_result = G_result.reshape([1500,1,28,28])\n",
        "\n",
        "mis = MNIST_IS(G_result)\n",
        "print('Inception Score:','mean:',mis[0],'std:',mis[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhP1F3hCP3qP"
      },
      "source": [
        "##### Visualizing GAN samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJSAa7TuP3qP"
      },
      "source": [
        "# Samples from G:\n",
        "z_ = torch.randn((1500, k))\n",
        "z_ = Variable(z_)\n",
        "if gpu_boole:\n",
        "    z_ = z_.cuda()\n",
        "if cnn_boole:\n",
        "    z_ = z_.view(-1, k, 1, 1) #needed for CNN        \n",
        "\n",
        "G_result = G(z_)\n",
        "G_result = G_result.cpu().data.numpy()\n",
        "G_result = G_result.reshape([1500,28,28])\n",
        "\n",
        "plt.imshow(G_result[0],cmap='gray')\n",
        "\n",
        "size_figure_grid = 5\n",
        "fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(5, 5))\n",
        "for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n",
        "    ax[i, j].get_xaxis().set_visible(False)\n",
        "    ax[i, j].get_yaxis().set_visible(False)\n",
        "\n",
        "for kr in range(5*5):\n",
        "    i = kr // 5\n",
        "    j = kr % 5\n",
        "    ax[i, j].cla()\n",
        "    ax[i, j].imshow(G_result[kr], cmap='gray')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}